{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6sa3cAnGjfl"
   },
   "source": [
    "# Region-based convolutional neural networks (R-CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFjtG7PuGjfm"
   },
   "source": [
    "### Overview\n",
    "\n",
    "Fine-tune a PyTorch R-CNN model to extract text from natural scenes (see figure below).   \n",
    "\n",
    "Prepare:\n",
    "  1. validation figures that show machine-predicted bounding boxes vs. the ground-truth boxes\n",
    "  1. loss-curves that show component-wise R-CNN losses as a function of epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UObe4TtwGjfm"
   },
   "source": [
    "![Text regions](attachment:4_images_with_boxes_and_text-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QEvLp8nRGjfm"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Train your model using the **Text** MS-COCO dataset.  Text MS-COCO is a subset of the original MS-COCO dataset (2014) with additional text annotations.  Read more: [COCO Text](https://bgshih.github.io/cocotext/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19127,
     "status": "ok",
     "timestamp": 1727138375944,
     "user": {
      "displayName": "Yuming Li",
      "userId": "08057734910641611309"
     },
     "user_tz": 420
    },
    "id": "HZzD71stIJoQ",
    "outputId": "00b6db78-28e0-4d7a-f868-00c6628670ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lTOBdrRGjfn"
   },
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5569,
     "status": "ok",
     "timestamp": 1727138383991,
     "user": {
      "displayName": "Yuming Li",
      "userId": "08057734910641611309"
     },
     "user_tz": 420
    },
    "id": "CcMtH23AGjfn"
   },
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "import numpy as np\n",
    "import skimage.io as io\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from torch.optim import SGD, Adam\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.rpn import AnchorGenerator, RPNHead, RegionProposalNetwork\n",
    "import torch\n",
    "import shutil\n",
    "import time\n",
    "from torch.cuda.amp import autocast, GradScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 247,
     "status": "ok",
     "timestamp": 1727138388137,
     "user": {
      "displayName": "Yuming Li",
      "userId": "08057734910641611309"
     },
     "user_tz": 420
    },
    "id": "Avw2KF4MGjfn"
   },
   "outputs": [],
   "source": [
    "#@title COCO_Text class: Handles the annotation file\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "__author__ = 'andreasveit'\n",
    "__version__ = '1.1'\n",
    "# Interface for accessing the COCO-Text dataset.\n",
    "\n",
    "# COCO-Text is a large dataset designed for text detection and recognition.\n",
    "# This is a Python API that assists in loading, parsing and visualizing the\n",
    "# annotations. The format of the COCO-Text annotations is also described on\n",
    "# the project website http://vision.cornell.edu/se3/coco-text/. In addition to this API, please download both\n",
    "# the COCO images and annotations.\n",
    "# This dataset is based on Microsoft COCO. Please visit http://mscoco.org/\n",
    "# for more information on COCO, including for the image data, object annotatins\n",
    "# and caption annotations.\n",
    "\n",
    "# An alternative to using the API is to load the annotations directly\n",
    "# into Python dictionary:\n",
    "# with open(annotation_filename) as json_file:\n",
    "#     coco_text = json.load(json_file)\n",
    "# Using the API provides additional utility functions.\n",
    "\n",
    "# The following API functions are defined:\n",
    "#  COCO_Text  - COCO-Text api class that loads COCO annotations and prepare data structures.\n",
    "#  getAnnIds  - Get ann ids that satisfy given filter conditions.\n",
    "#  getImgIds  - Get img ids that satisfy given filter conditions.\n",
    "#  loadAnns   - Load anns with the specified ids.\n",
    "#  loadImgs   - Load imgs with the specified ids.\n",
    "#  showAnns   - Display the specified annotations.\n",
    "#  loadRes    - Load algorithm results and create API for accessing them.\n",
    "# Throughout the API \"ann\"=annotation, \"cat\"=category, and \"img\"=image.\n",
    "\n",
    "# COCO-Text Toolbox.        Version 1.1\n",
    "# Data and  paper available at:  http://vision.cornell.edu/se3/coco-text/\n",
    "# Code based on Microsoft COCO Toolbox Version 1.0 by Piotr Dollar and Tsung-Yi Lin\n",
    "# extended and adapted by Andreas Veit, 2016.\n",
    "# Licensed under the Simplified BSD License [see bsd.txt]\n",
    "\n",
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.collections import PatchCollection\n",
    "from matplotlib.patches import Rectangle, PathPatch\n",
    "from matplotlib.path import Path\n",
    "import numpy as np\n",
    "import copy\n",
    "import os\n",
    "\n",
    "class COCO_Text:\n",
    "    def __init__(self, annotation_file=None):\n",
    "        \"\"\"\n",
    "        Constructor of COCO-Text helper class for reading and visualizing annotations.\n",
    "        :param annotation_file (str): location of annotation file\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # load dataset\n",
    "        self.dataset = {}\n",
    "        self.anns = {}\n",
    "        self.imgToAnns = {}\n",
    "        self.catToImgs = {}\n",
    "        self.imgs = {}\n",
    "        self.cats = {}\n",
    "        self.val = []\n",
    "        self.test = []\n",
    "        self.train = []\n",
    "        if not annotation_file == None:\n",
    "            assert os.path.isfile(annotation_file), \"file does not exist\"\n",
    "            print('loading annotations into memory...')\n",
    "            time_t = datetime.datetime.utcnow()\n",
    "            dataset = json.load(open(annotation_file, 'r'))\n",
    "            print(datetime.datetime.utcnow() - time_t)\n",
    "            self.dataset = dataset\n",
    "            self.createIndex()\n",
    "\n",
    "    def createIndex(self):\n",
    "        # create index\n",
    "        print('creating index...')\n",
    "        self.imgToAnns = {int(cocoid): self.dataset['imgToAnns'][cocoid] for cocoid in self.dataset['imgToAnns']}\n",
    "        self.imgs      = {int(cocoid): self.dataset['imgs'][cocoid] for cocoid in self.dataset['imgs']}\n",
    "        self.anns      = {int(annid): self.dataset['anns'][annid] for annid in self.dataset['anns']}\n",
    "        self.cats      = self.dataset['cats']\n",
    "        self.val       = [int(cocoid) for cocoid in self.dataset['imgs'] if self.dataset['imgs'][cocoid]['set'] == 'val']\n",
    "        self.test      = [int(cocoid) for cocoid in self.dataset['imgs'] if self.dataset['imgs'][cocoid]['set'] == 'test']\n",
    "        self.train     = [int(cocoid) for cocoid in self.dataset['imgs'] if self.dataset['imgs'][cocoid]['set'] == 'train']\n",
    "        print('index created!')\n",
    "\n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Print information about the annotation file.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        for key, value in self.dataset['info'].items():\n",
    "            print('%s: %s'%(key, value))\n",
    "\n",
    "    def filtering(self, filterDict, criteria):\n",
    "        return [key for key in filterDict if all(criterion(filterDict[key]) for criterion in criteria)]\n",
    "\n",
    "    def getAnnByCat(self, properties):\n",
    "        \"\"\"\n",
    "        Get ann ids that satisfy given properties\n",
    "        :param properties (list of tuples of the form [(category type, category)] e.g., [('readability','readable')]\n",
    "            : get anns for given categories - anns have to satisfy all given property tuples\n",
    "        :return: ids (int array)       : integer array of ann ids\n",
    "        \"\"\"\n",
    "        return self.filtering(self.anns, [lambda d, x=a, y=b:d[x] == y for (a,b) in properties])\n",
    "\n",
    "    def getAnnIds(self, imgIds=[], catIds=[], areaRng=[]):\n",
    "        \"\"\"\n",
    "        Get ann ids that satisfy given filter conditions. default skips that filter\n",
    "        :param imgIds  (int array)     : get anns for given imgs\n",
    "               catIds  (list of tuples of the form [(category type, category)] e.g., [('readability','readable')]\n",
    "                : get anns for given cats\n",
    "               areaRng (float array)   : get anns for given area range (e.g. [0 inf])\n",
    "        :return: ids (int array)       : integer array of ann ids\n",
    "        \"\"\"\n",
    "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
    "        catIds = catIds if type(catIds) == list else [catIds]\n",
    "\n",
    "        if len(imgIds) == len(catIds) == len(areaRng) == 0:\n",
    "            anns = list(self.anns.keys())\n",
    "        else:\n",
    "            if not len(imgIds) == 0:\n",
    "                anns = sum([self.imgToAnns[imgId] for imgId in imgIds if imgId in self.imgToAnns],[])\n",
    "            else:\n",
    "                anns = list(self.anns.keys())\n",
    "            anns = anns if len(catIds)  == 0 else list(set(anns).intersection(set(self.getAnnByCat(catIds))))\n",
    "            anns = anns if len(areaRng) == 0 else [ann for ann in anns if self.anns[ann]['area'] > areaRng[0] and self.anns[ann]['area'] < areaRng[1]]\n",
    "        return anns\n",
    "\n",
    "    def getImgIds(self, imgIds=[], catIds=[]):\n",
    "        '''\n",
    "        Get img ids that satisfy given filter conditions.\n",
    "        :param imgIds (int array) : get imgs for given ids\n",
    "        :param catIds (int array) : get imgs with all given cats\n",
    "        :return: ids (int array)  : integer array of img ids\n",
    "        '''\n",
    "        imgIds = imgIds if type(imgIds) == list else [imgIds]\n",
    "        catIds = catIds if type(catIds) == list else [catIds]\n",
    "\n",
    "        if len(imgIds) == len(catIds) == 0:\n",
    "            ids = list(self.imgs.keys())\n",
    "        else:\n",
    "            ids = set(imgIds)\n",
    "            if not len(catIds) == 0:\n",
    "                ids  = ids.intersection(set([self.anns[annid]['image_id'] for annid in self.getAnnByCat(catIds)]))\n",
    "        return list(ids)\n",
    "\n",
    "    def loadAnns(self, ids=[]):\n",
    "        \"\"\"\n",
    "        Load anns with the specified ids.\n",
    "        :param ids (int array)       : integer ids specifying anns\n",
    "        :return: anns (object array) : loaded ann objects\n",
    "        \"\"\"\n",
    "        if type(ids) == list:\n",
    "            return [self.anns[id] for id in ids]\n",
    "        elif type(ids) == int:\n",
    "            return [self.anns[ids]]\n",
    "\n",
    "    def loadImgs(self, ids=[]):\n",
    "        \"\"\"\n",
    "        Load anns with the specified ids.\n",
    "        :param ids (int array)       : integer ids specifying img\n",
    "        :return: imgs (object array) : loaded img objects\n",
    "        \"\"\"\n",
    "        if type(ids) == list:\n",
    "            return [self.imgs[id] for id in ids]\n",
    "        elif type(ids) == int:\n",
    "            return [self.imgs[ids]]\n",
    "\n",
    "    def showAnns(self, anns, show_polygon=False):\n",
    "        \"\"\"\n",
    "        Display the specified annotations.\n",
    "        :param anns (array of object): annotations to display\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        if len(anns) == 0:\n",
    "            return 0\n",
    "        ax = plt.gca()\n",
    "        boxes = []\n",
    "        color = []\n",
    "        for ann in anns:\n",
    "            c = np.random.random((1, 3)).tolist()[0]\n",
    "            if show_polygon:\n",
    "                tl_x, tl_y, tr_x, tr_y, br_x, br_y, bl_x, bl_y = ann['polygon']\n",
    "                verts = [(tl_x, tl_y), (tr_x, tr_y), (br_x, br_y), (bl_x, bl_y), (0, 0)]\n",
    "                codes = [Path.MOVETO, Path.LINETO, Path.LINETO, Path.LINETO, Path.CLOSEPOLY]\n",
    "                path = Path(verts, codes)\n",
    "                patch = PathPatch(path, facecolor='none')\n",
    "                boxes.append(patch)\n",
    "                left, top = tl_x, tl_y\n",
    "            else:\n",
    "                left, top, width, height = ann['bbox']\n",
    "                boxes.append(Rectangle([left,top],width,height,alpha=0.4))\n",
    "            color.append(c)\n",
    "            if 'utf8_string' in list(ann.keys()):\n",
    "                ax.annotate(ann['utf8_string'],(left,top-4),color=c)\n",
    "        p = PatchCollection(boxes, facecolors=color, edgecolors=(0,0,0,1), linewidths=3, alpha=0.4)\n",
    "        ax.add_collection(p)\n",
    "\n",
    "    def loadRes(self, resFile):\n",
    "        \"\"\"\n",
    "        Load result file and return a result api object.\n",
    "        :param   resFile (str)     : file name of result file\n",
    "        :return: res (obj)         : result api object\n",
    "        \"\"\"\n",
    "        res = COCO_Text()\n",
    "        res.dataset['imgs'] = [img for img in self.dataset['imgs']]\n",
    "\n",
    "        print('Loading and preparing results...     ')\n",
    "        time_t = datetime.datetime.utcnow()\n",
    "        if type(resFile) == str:\n",
    "            anns = json.load(open(resFile))\n",
    "        else:\n",
    "            anns = resFile\n",
    "        assert type(anns) == list, 'results in not an array of objects'\n",
    "        annsImgIds = [int(ann['image_id']) for ann in anns]\n",
    "\n",
    "        if set(annsImgIds) != (set(annsImgIds) & set(self.getImgIds())):\n",
    "            print('Results do not correspond to current coco set')\n",
    "            print('skipping ', str(len(set(annsImgIds)) - len(set(annsImgIds) & set(self.getImgIds()))), ' images')\n",
    "        annsImgIds = list(set(annsImgIds) & set(self.getImgIds()))\n",
    "\n",
    "        res.imgToAnns = {cocoid : [] for cocoid in annsImgIds}\n",
    "        res.imgs = {cocoid: self.imgs[cocoid] for cocoid in annsImgIds}\n",
    "\n",
    "        assert anns[0]['bbox'] != [], 'results have incorrect format'\n",
    "        for id, ann in enumerate(anns):\n",
    "            if ann['image_id'] not in annsImgIds:\n",
    "                continue\n",
    "            bb = ann['bbox']\n",
    "            ann['area'] = bb[2]*bb[3]\n",
    "            ann['id'] = id\n",
    "            res.anns[id] = ann\n",
    "            res.imgToAnns[ann['image_id']].append(id)\n",
    "        print('DONE (t=%0.2fs)'%((datetime.datetime.utcnow() - time_t).total_seconds()))\n",
    "\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OS67T_nBGjfo"
   },
   "source": [
    "### CocoDataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 228,
     "status": "ok",
     "timestamp": 1727138393042,
     "user": {
      "displayName": "Yuming Li",
      "userId": "08057734910641611309"
     },
     "user_tz": 420
    },
    "id": "XQg0KwdyGjfo"
   },
   "outputs": [],
   "source": [
    "#@title Dataset class: use images and annotation file to implement a dataset that can used with Dataloader\n",
    "\n",
    "class CocoDataset(Dataset):\n",
    "    def __init__(self, root_dir, annFile, transform=None, cuda=True):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.imgs = os.listdir(root_dir)\n",
    "        # annotations\n",
    "        self.ct = COCO_Text(annFile)\n",
    "        self.imgIds = self.ct.getImgIds(imgIds=self.ct.train,\n",
    "                    catIds=[('legibility','legible'),('class','machine printed')])\n",
    "\n",
    "        for imgId in self.imgIds:\n",
    "            file_name = self.ct.loadImgs(imgId)[0]['file_name']\n",
    "            if file_name not in self.imgs:\n",
    "                self.imgIds.remove(imgId)\n",
    "        # manual exclude\n",
    "        self.imgIds.remove(275939)\n",
    "        self.imgIds.remove(443671)\n",
    "\n",
    "        # remaining images\n",
    "        print(f\"remaining images in ann file: {len(self.imgIds)}, remaining images in folder: {len(self.imgs)}\")\n",
    "\n",
    "        self.imgIds.sort()\n",
    "        # sort the images in same order as the annotations\n",
    "        self.imgs = [self.ct.loadImgs(imgId)[0]['file_name'] for imgId in self.imgIds]\n",
    "\n",
    "        self.img_h = 224\n",
    "        self.img_w = 224\n",
    "        self.cuda = cuda\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. Get the image ID\n",
    "        img_id = self.imgIds[idx]\n",
    "\n",
    "        # 2. Load image file\n",
    "        img_info = self.ct.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.root_dir, img_info['file_name'])\n",
    "        image = Image.open(img_path).convert(\"RGB\")  # Ensure image is in RGB format\n",
    "\n",
    "        # Get the original image size before any resizing\n",
    "        original_width, original_height = image.size\n",
    "\n",
    "        # 3. Get annotation IDs for the image\n",
    "        ann_ids = self.ct.getAnnIds(imgIds=[img_id])\n",
    "        anns = self.ct.loadAnns(ann_ids)\n",
    "\n",
    "        # 4. Initialize lists to hold bounding boxes and labels\n",
    "        boxes = []\n",
    "        labels = []\n",
    "\n",
    "        # 5. Process each annotation\n",
    "        for ann in anns:\n",
    "            # Get bbox (in COCO format: [x_min, y_min, width, height])\n",
    "            bbox = ann['bbox']\n",
    "            xmin, ymin, width, height = bbox\n",
    "            xmax = xmin + width\n",
    "            ymax = ymin + height\n",
    "\n",
    "            # Append the bbox in [xmin, ymin, xmax, ymax] format\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            # For text detection, all annotations are labeled as \"text\" (class 1)\n",
    "            label = 1  # 1 represents \"text\" class\n",
    "            labels.append(label)\n",
    "\n",
    "        # 6. Convert to torch tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)  # Convert labels to integer tensor\n",
    "\n",
    "        # 7. Create the target dictionary\n",
    "        target = {\n",
    "            \"boxes\": boxes,\n",
    "            \"labels\": labels,\n",
    "            \"image_id\": torch.tensor([img_id])\n",
    "        }\n",
    "\n",
    "        # 8. Apply transformations if any (e.g., data augmentation, resizing)\n",
    "        if self.transform:\n",
    "            # Apply the transformations to the image\n",
    "            image = self.transform(image)\n",
    "\n",
    "            # If resizing, we need to adjust bounding boxes based on new image size\n",
    "            # After transformation, image is now in tensor format with [C, H, W]\n",
    "            _, new_height, new_width = image.shape\n",
    "\n",
    "            # Calculate scaling factors based on new size\n",
    "            scale_x = new_width / original_width\n",
    "            scale_y = new_height / original_height\n",
    "\n",
    "            # Adjust the bounding boxes based on scaling\n",
    "            boxes = target['boxes']\n",
    "            boxes[:, [0, 2]] *= scale_x  # Scale xmin and xmax\n",
    "            boxes[:, [1, 3]] *= scale_y  # Scale ymin and ymax\n",
    "\n",
    "            # Update the target dictionary with scaled bounding boxes\n",
    "            target['boxes'] = boxes\n",
    "\n",
    "        return image, target\n",
    "\n",
    "\n",
    "\n",
    "# coalate_fn is used to collate the data into batches\n",
    "def collate_fn(batch):\n",
    "    images = []\n",
    "    targets = []\n",
    "    for item in batch:\n",
    "        images.append(item[0])\n",
    "        targets.append(item[1])\n",
    "    images = torch.stack(images, 0)\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZX3LlQkGjfp"
   },
   "source": [
    "# Evaluate / region visualization\n",
    "\n",
    "Load four images and predict regions using model.\n",
    "Create a figure that shows the images as subplots.\n",
    "Overlay model predicted regions (blue) and target regions (red).\n",
    "Save the output as an image -- use epoch in filename to preserve order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 189,
     "status": "ok",
     "timestamp": 1727138398178,
     "user": {
      "displayName": "Yuming Li",
      "userId": "08057734910641611309"
     },
     "user_tz": 420
    },
    "id": "noAcYoVzGjfp"
   },
   "outputs": [],
   "source": [
    "# def evaluate(model, dataloader, device, epoch):\n",
    "#     # TODO: create four-panel subplots showing bounding boxes\n",
    "\n",
    "#     raise NotImplementedError(\"evaluate()\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import torch\n",
    "\n",
    "def denormalize(image, mean, std):\n",
    "    \"\"\"Denormalize an image tensor that has been normalized using mean and std.\"\"\"\n",
    "    image = image.clone().cpu()\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)  # Reverse the normalization: x' = (x * std) + mean\n",
    "    return image\n",
    "\n",
    "def evaluate(model, dataloader, device, epoch):\n",
    "    \"\"\"\n",
    "    Evaluates the model on four images and visualizes the predicted and target bounding boxes.\n",
    "\n",
    "    Args:\n",
    "        model: The Faster R-CNN model.\n",
    "        dataloader: The DataLoader for the dataset.\n",
    "        device: The device to run the model on (e.g., 'cuda' or 'cpu').\n",
    "        epoch: The current epoch, used to save output with appropriate filename.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Select four images from the dataloader\n",
    "    images, targets = next(iter(dataloader))\n",
    "\n",
    "    # Move images and model to the specified device\n",
    "    images = [img.to(device) for img in images]\n",
    "    model.to(device)\n",
    "\n",
    "    # Perform inference using the pre-trained model (no gradients required)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(images)\n",
    "\n",
    "    # Mean and std used for normalization (ImageNet values)\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    # Create a figure for the 4 subplots\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n",
    "\n",
    "    for i in range(4):\n",
    "        # Denormalize the image for visualization\n",
    "        img = denormalize(images[i], mean, std).permute(1, 2, 0).numpy()  # [C, H, W] -> [H, W, C]\n",
    "        img = (img * 255).astype('uint8')  # Convert to uint8 for proper display\n",
    "\n",
    "        # Display image\n",
    "        axes[i].imshow(img)\n",
    "\n",
    "        # Plot target (ground truth) boxes in red - these boxes should NOT be normalized\n",
    "        for box in targets[i]['boxes']:\n",
    "            xmin, ymin, xmax, ymax = box.cpu().numpy()\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                     linewidth=2, edgecolor='r', facecolor='none')\n",
    "            axes[i].add_patch(rect)\n",
    "\n",
    "        # Plot predicted boxes in blue - these are also based on the original pixel values\n",
    "        for box in predictions[i]['boxes']:\n",
    "            xmin, ymin, xmax, ymax = box.cpu().numpy()\n",
    "            rect = patches.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
    "                                     linewidth=2, edgecolor='b', facecolor='none')\n",
    "            axes[i].add_patch(rect)\n",
    "\n",
    "        # Remove axis labels for cleaner visualization\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    # Save the figure as an image\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'/content/drive/MyDrive/track_image/evaluation_epoch_{epoch}.png')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UibNlbedGjfq"
   },
   "source": [
    "# Model factory\n",
    "\n",
    "[Example](https://pytorch.org/tutorials/intermediate/torchvision_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1727138401742,
     "user": {
      "displayName": "Yuming Li",
      "userId": "08057734910641611309"
     },
     "user_tz": 420
    },
    "id": "V-t8cmbcGjfq"
   },
   "outputs": [],
   "source": [
    "# def get_model():\n",
    "#     # TODO: create and return a PyTorch model\n",
    "\n",
    "#     raise NotImplementedError(\"get_model()\")\n",
    "\n",
    "def get_model(num_classes=2):\n",
    "    \"\"\"\n",
    "    Load a pretrained Faster R-CNN model and modify it for the COCO-Text dataset.\n",
    "\n",
    "    Args:\n",
    "        num_classes (int): The number of classes (including background) in your dataset.\n",
    "\n",
    "    Returns:\n",
    "        model: A modified Faster R-CNN model ready for training on your dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the pre-trained Faster R-CNN model on COCO dataset\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # Get the number of input features for the classifier (this is the final layer of Faster R-CNN)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "\n",
    "    # Replace the pre-trained head with a new one (num_classes includes the background class)\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLnpDFL3Gjfq"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 43136,
     "status": "ok",
     "timestamp": 1727138446824,
     "user": {
      "displayName": "Yuming Li",
      "userId": "08057734910641611309"
     },
     "user_tz": 420
    },
    "id": "Qo_OI5fuGjfq",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "c7aa94f9-d3d2-4aec-c55a-cfdf07746ba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "0:00:03.538181\n",
      "creating index...\n",
      "index created!\n",
      "remaining images in ann file: 8738, remaining images in folder: 8738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:02<00:00, 78.9MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform = transforms.Compose([\n",
    "#   # TODO: e.g. transforms.Normalize()\n",
    "# ])\n",
    "\n",
    "# # TODO: dataset = CocoDataset(..., transform=transform)\n",
    "# dataset = None\n",
    "# # TODO: dataloader = DataLoader(..., collate_fn=collate_fn)\n",
    "# dataloader = None\n",
    "\n",
    "# model = get_model()\n",
    "# # revise as needed\n",
    "# device = torch.device('cpu')\n",
    "# model.to(device)\n",
    "\n",
    "root_dir = \"/content/drive/MyDrive/EE641_Dataset/mscoco_text_cleaned_v01/data/train\"\n",
    "annFile = \"/content/drive/MyDrive/EE641_Dataset/COCO_Text.json\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize all images to 224x224\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = CocoDataset(root_dir, annFile, transform=transform, cuda=True)\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=8, collate_fn=collate_fn)\n",
    "dataloader_evaluate = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = get_model(num_classes=2)  # For text detection (2 classes: background + text)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uATZYp7Gjfq"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1wDOU9BnhQ5XHdBB5ndM-uLqqBHfcAYaW"
    },
    "executionInfo": {
     "elapsed": 23271679,
     "status": "ok",
     "timestamp": 1727167835625,
     "user": {
      "displayName": "Yuming Li",
      "userId": "08057734910641611309"
     },
     "user_tz": 420
    },
    "id": "y5XmO5soGjfq",
    "outputId": "ff68f8b9-148b-407d-91af-f33dd055d35d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output hidden; open in https://colab.research.google.com to view."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # TODO: complete optimizer (optional, learning rate scheduler)\n",
    "# # optimizer =\n",
    "\n",
    "# num_epochs = 100\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for idx, data in enumerate(dataloader):\n",
    "#         model.train()\n",
    "\n",
    "#         # TODO: implement training loop: inference, backpropagate, update\n",
    "\n",
    "#         # https://pytorch.org/vision/stable/models/faster_rcnn.html\n",
    "#         #\n",
    "#         # See: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py\n",
    "#         #   class FasterRCNN\n",
    "\n",
    "#         # TODO: track componentwise (4) RCNN loss vs. epoch.\n",
    "\n",
    "\n",
    "#     # TODO: checkpoint model weights, optimizer state, and other as needed to resume\n",
    "#     # TODO: plot each of the 4 RCNN loss components vs epoch-number\n",
    "\n",
    "#     # VALIDATE\n",
    "#     model.eval()\n",
    "#     evaluate(model, dataloader, device, epoch)\n",
    "\n",
    "# Define optimizer and optional learning rate scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "# Optionally, define a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Define number of epochs\n",
    "num_epochs = 100\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "# Initialize lists to store loss components for plotting later\n",
    "loss_classifier_list = []\n",
    "loss_box_reg_list = []\n",
    "loss_objectness_list = []\n",
    "loss_rpn_box_reg_list = []\n",
    "\n",
    "# # Stepwise unfreezing of RPN and Backbone\n",
    "# def freeze_rpn_backbone(model):\n",
    "#     \"\"\" Freeze RPN and Backbone parameters. \"\"\"\n",
    "#     for param in model.backbone.parameters():\n",
    "#         param.requires_grad = False\n",
    "#     for param in model.rpn.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "# def unfreeze_rpn(model):\n",
    "#     \"\"\" Unfreeze only RPN, keep Backbone frozen. \"\"\"\n",
    "#     for param in model.rpn.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# def unfreeze_backbone(model):\n",
    "#     \"\"\" Unfreeze Backbone. \"\"\"\n",
    "#     for param in model.backbone.parameters():\n",
    "#         param.requires_grad = True\n",
    "\n",
    "# # Start with RPN and Backbone frozen\n",
    "# freeze_rpn_backbone(model)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # # Unfreeze RPN after 5 epochs\n",
    "    # if epoch == 2:\n",
    "    #     print(f\"Unfreezing RPN at epoch {epoch}\")\n",
    "    #     unfreeze_rpn(model)\n",
    "\n",
    "    # # Unfreeze Backbone after 10 epochs\n",
    "    # if epoch == 3:\n",
    "    #     print(f\"Unfreezing Backbone at epoch {epoch}\")\n",
    "    #     unfreeze_backbone(model)\n",
    "\n",
    "    model.train()  # Set model to training mode\n",
    "    epoch_loss = {'loss_classifier': 0, 'loss_box_reg': 0, 'loss_objectness': 0, 'loss_rpn_box_reg': 0}\n",
    "\n",
    "    # Training loop over batches\n",
    "    for idx, (images, targets) in enumerate(dataloader):\n",
    "        # Move images and targets to device (CPU or GPU)\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Zero the gradients before the backward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "        # Forward pass (inference)\n",
    "          loss_dict = model(images, targets)\n",
    "\n",
    "          # Compute total loss by summing the individual loss components\n",
    "          total_loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Backward pass (backpropagation) with mixed precision scaling\n",
    "        scaler.scale(total_loss).backward()\n",
    "\n",
    "        # Update model weights\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        # Track losses for each component\n",
    "        for key in epoch_loss:\n",
    "            epoch_loss[key] += loss_dict[key].item()\n",
    "\n",
    "        # Every 500 batches, print losses\n",
    "        if idx % 50 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}], Batch [{idx}/{len(dataloader)}]\")\n",
    "            print(f\"Total Loss: {total_loss.item():.4f}, \"\n",
    "                  f\"Classifier Loss: {loss_dict['loss_classifier'].item():.4f}, \"\n",
    "                  f\"Box Reg Loss: {loss_dict['loss_box_reg'].item():.4f}, \"\n",
    "                  f\"Objectness Loss: {loss_dict['loss_objectness'].item():.4f}, \"\n",
    "                  f\"RPN Box Reg Loss: {loss_dict['loss_rpn_box_reg'].item():.4f}\")\n",
    "\n",
    "    # Average the loss components for the epoch\n",
    "    num_batches = len(dataloader)\n",
    "    for key in epoch_loss:\n",
    "        epoch_loss[key] /= num_batches\n",
    "\n",
    "    # Store loss values for plotting\n",
    "    loss_classifier_list.append(epoch_loss['loss_classifier'])\n",
    "    loss_box_reg_list.append(epoch_loss['loss_box_reg'])\n",
    "    loss_objectness_list.append(epoch_loss['loss_objectness'])\n",
    "    loss_rpn_box_reg_list.append(epoch_loss['loss_rpn_box_reg'])\n",
    "\n",
    "    # Step the learning rate scheduler if defined\n",
    "    if lr_scheduler:\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        # Checkpoint model weights, optimizer state, and epoch\n",
    "        checkpoint = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'scaler_state_dict': scaler.state_dict()\n",
    "        }\n",
    "        torch.save(checkpoint, f\"/content/drive/MyDrive/check_point/checkpoint_epoch_{epoch}.pth\")\n",
    "\n",
    "    # VALIDATE (run evaluation)\n",
    "    model.eval()\n",
    "    evaluate(model, dataloader_evaluate, device, epoch)\n",
    "\n",
    "# Plot the 4 RCNN loss components over epochs\n",
    "epochs_range = range(1, num_epochs + 1)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs_range, loss_classifier_list, label='Classifier Loss')\n",
    "plt.plot(epochs_range, loss_box_reg_list, label='Box Regression Loss')\n",
    "plt.plot(epochs_range, loss_objectness_list, label='Objectness Loss')\n",
    "plt.plot(epochs_range, loss_rpn_box_reg_list, label='RPN Box Regression Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"RCNN Loss Components Over Epochs\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (ee541)",
   "language": "python",
   "name": "ee541"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8ee5a1dfcde07afddddcf032bbff525613c7ab779fc1737487cb945fc6b7fc63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
